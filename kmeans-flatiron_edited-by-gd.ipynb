{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Introduction to $k$-means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### By the end of this lecture, students will be able to:\n",
    "\n",
    "- **Assess** what scenarios could use $k$-means\n",
    "\n",
    "- **Articulate** the methodology used by $k$-means\n",
    "\n",
    "- **Apply** KMeans from sklearn.cluster to a relevant dataset\n",
    "\n",
    "- **Select** the appropriate number of clusters using $k$-means and the elbow method\n",
    "\n",
    "- **Evaluate** the weaknesses and remedies to $k$-means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Scenario\n",
    "\n",
    ">You work for the marketing department within a large company that manages a customer base. \n",
    "For each customer you have a record of average purchase cost and time since last purchase.<br> \n",
    "You know that if you want to retain your customers you cannot treat them the same. You can use targeted marketing ads towards groups that demonstrate different behavior, but how will you divide the customers into groups?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## **Part 1**: Concept introduction\n",
    "#### Import libraries and download dataset\n",
    "\n",
    "We are continuing to use Scikit Learn as our main library.\n",
    "The specific documentation for k-means can be found [here](http://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# Required packages for today\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn import metrics\n",
    "from sklearn import datasets\n",
    "\n",
    "# Familiar packages for plotting, data manipulation, and numeric functions\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# import Alison's code for the demo clusters\n",
    "from demo_images import *\n",
    "\n",
    "# Have plots appear in notebook\n",
    "%matplotlib inline\n",
    "\n",
    "# Default plot params\n",
    "plt.style.use('seaborn')\n",
    "cmap = 'tab10'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Clustering!   Finding **GROUPS**\n",
    "\n",
    "How many groups do you see?\n",
    "\n",
    "![img](img/initialscenario.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Wait - How is clustering different from classification?\n",
    "\n",
    ">In _classification_ you **know** what groups are in the dataset and the goal is to _**predict**_ class membership accurately.\n",
    "\n",
    ">In _clustering_ you **do not** know which groups are in the dataset and you are trying to _**identify**_ the groups."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### So what do you do with clustering results?\n",
    "\n",
    "Clustering is often an *informing* step in your analysis. Once clusters are identified, one can:\n",
    "- Create strategies on how to approach each group differently\n",
    "- Use cluster membership as an independent variable in a predictive model\n",
    "- Use the clusters as the _**target label**_ in future classification models. How would you assign new data to the existing clusters?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Explore the algorithm with an intuitive K means approach\n",
    "\n",
    "### Observe the following four methods with a sample dataset:\n",
    "\n",
    "### Method Questions:\n",
    "\n",
    "- What do they have in common?\n",
    "- What are the differences between them?\n",
    "- How many groups are there in the end?\n",
    "- Do you see any problems with this method?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Method 1\n",
    "\n",
    "![left](img/from-left.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Method 2\n",
    "\n",
    "![right](img/from-right.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Method 3\n",
    "\n",
    "![top](img/from-top.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Method 4\n",
    "\n",
    "![bottom](img/from-bottom.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Review Method Questions:\n",
    "\n",
    "- What do they have in common?\n",
    "- What are the differences between them?\n",
    "- How many groups are there in the end?\n",
    "- Do you see any problems with this method?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "In common:\n",
    "- Green dots starts at points\n",
    "- Calculates distance\n",
    "- Moves dots\n",
    "- Re-measures distance\n",
    "- Moves dots as needed\n",
    "\n",
    "\n",
    "Differences:\n",
    "- Dots start in different places and groups settle in different places\n",
    "\n",
    "Groups:\n",
    "- There are four groups\n",
    "\n",
    "Problem with this method?\n",
    "- Too variable!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### K-means algorithm, at its core, in an optimization function\n",
    "\n",
    "![minmax](img/minmaxdata.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Reassigns groups and adjusts centroids to...\n",
    "![min](img/min.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### And to...\n",
    "![max](img/max.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Sci-kit Learn** documentation actually has some pretty good [documentation describing the algorithm](https://scikit-learn.org/stable/modules/clustering.html#k-mean) if you wish for more detail."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Data for the exercise\n",
    "\n",
    "- This is a sample dataset. \n",
    "- Let us assume the data is already scaled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_dat = pd.read_csv(\"data/xclara.txt\", header=0, index_col=0)\n",
    "dummy_dat.reset_index(inplace=True)\n",
    "dummy_dat.drop('index', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_dat.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_dat.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### EDA of variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_dat.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(dummy_dat['V1'], dummy_dat['V2']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Introduction of `Kmeans`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KMeans(n_clusters=3).fit(dummy_dat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the `init` and `n_init` parameters!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.cluster_centers_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(dummy_dat['V1'], dummy_dat['V2'])\n",
    "for i in range(len(model.cluster_centers_)):\n",
    "    plt.scatter(model.cluster_centers_[i][0],\n",
    "                model.cluster_centers_[i][1]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict([[60, -20]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(dummy_dat['V1'], dummy_dat['V2'], c=model.labels_);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_df = pd.concat([dummy_dat, pd.DataFrame(model.labels_,\n",
    "                        columns=['cluster'])], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster0 = labeled_df[labeled_df['cluster'] == 0]\n",
    "cluster1 = labeled_df[labeled_df['cluster'] == 1]\n",
    "cluster2 = labeled_df[labeled_df['cluster'] == 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#type(cluster2['V2'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster0['V1'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(cluster0['V1'], cluster0['V2'], c='k', alpha=0.4)\n",
    "plt.scatter(cluster1['V1'], cluster1['V2'], c='c', alpha=0.4)\n",
    "plt.scatter(cluster2['V1'], cluster2['V2'], c='r', alpha=0.4);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Note! \n",
    "#### Do you have different cluster_centers?\n",
    "*Good!*\n",
    "\n",
    "We saw in the demo that the algorithm is sensitive to starting points.\n",
    "\n",
    "We can use the additional argument `random_state` to set the seed and have repeatable results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_setseed = KMeans(n_clusters=4, random_state=10).fit(dummy_dat)\n",
    "model_setseed.cluster_centers_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### **Exercise**: \n",
    "- Try running `Kmeans` with different numbers of `n_clusters`, $k=2$ through $k=7$\n",
    "- Check the `cluster_centers_` \n",
    "- Without running any more functions, which number of $k$ is the best?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Practice code goes here\n",
    "\n",
    "model2 = KMeans(n_clusters=2, random_state=42)\n",
    "model2.fit(dummy_dat)\n",
    "model2.cluster_centers_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model4 = KMeans(n_clusters=4, random_state=42)\n",
    "model4.fit(dummy_dat)\n",
    "model4.cluster_centers_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## **Part 2**: Choosing the appropriate number of $k$\n",
    "\n",
    "#### Two metrics we can use: **elbow method** and the **silhouette coefficient**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### **Part 2A**: Elbow Method\n",
    "\n",
    "Elbow method uses the sum of squared error calculated from each instance of $k$ to find the best value of $k$.\n",
    "\n",
    "This is sometimes called the \"inertia\" of the model, and fitted sklearn $k$-means models have an `inertia_` attribute.\n",
    "\n",
    "Sometimes you will see the SSE divided by the total sum of squares in the dataset (how far is each point from the center of the entire dataset)\n",
    "\n",
    "Fewer clusters seems better, but inertia will always decrease with _more_ clusters. Hence the idea of looking for an elbow in the plot of inertia vs. $k$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.inertia_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inertia is the sum of squared distances between points and their cluster center."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Specifying the dataset and initializing variables\n",
    "X = dummy_dat\n",
    "distortions = []\n",
    "\n",
    "# Calculate SSE for different K\n",
    "for k in range(2, 10):\n",
    "    kmeans = KMeans(n_clusters=k, random_state = 301)\n",
    "    kmeans.fit(X)\n",
    "    distortions.append(kmeans.inertia_)\n",
    "\n",
    "# Plot values of SSE\n",
    "plt.figure(figsize=(15,8))\n",
    "plt.subplot(121, title='Elbow curve')\n",
    "plt.xlabel('k')\n",
    "plt.plot(range(2, 10), distortions)\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### **Part 2B**: Silhouette Coefficient\n",
    "\n",
    "![silo](img/silo2.png)\n",
    "\n",
    "> **a** refers to the average distance between a point and all other points in that cluster.\n",
    "\n",
    "> **b** refers to the average distance between that same point and all other points in clusters to which it does not belong\n",
    "\n",
    "It is calculated for each point in the dataset, then averaged across all points for one cumulative score.\n",
    "\n",
    "The Silhouette Coefficient ranges between -1 and 1. The closer to 1, the more clearly defined are the clusters. The closer to -1, the more incorrect assignment.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose I had four points in a one-dimensional space: 0, 1, 9, and 10.\n",
    "\n",
    "Then we would calculate the Silhouette Score as follows:\n",
    "\n",
    "Point 0 and Point 10:\n",
    "- $a = 1$\n",
    "- $b = 9.5$\n",
    "- $score_{0, 10} = \\frac{9.5 - 1}{9.5} = \\frac{17}{19}$\n",
    "\n",
    "Point 1 and Point 9:\n",
    "- $a = 1$\n",
    "- $b = 8.5$\n",
    "- $score_{1, 9} = \\frac{8.5 - 1}{8.5} = \\frac{15}{17}$\n",
    "\n",
    "The full Silhouette Score would be the average of all of these individual scores:\n",
    "\n",
    "$\\large s = \\frac{2\\left(\\frac{17}{19}\\right) + 2\\left(\\frac{15}{17}\\right)}{4}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "(2 * 17/19 + 2 * 15/17) / 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.silhouette_score(np.array([0, 1, 9, 10]).reshape(-1, 1), ['red', 'red', 'blue', 'blue'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# Generate silhouette coefficient for each k\n",
    "X = dummy_dat\n",
    "silhouette_plot = []\n",
    "for k in range(2, 10):\n",
    "    clusters = KMeans(n_clusters=k, random_state=10)\n",
    "    cluster_labels = clusters.fit_predict(X)\n",
    "    silhouette_avg = metrics.silhouette_score(X, cluster_labels)\n",
    "    silhouette_plot.append(silhouette_avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Plot Silhouette coefficient\n",
    "plt.figure(figsize=(15,8))\n",
    "plt.subplot(121, title='Silhouette coefficients over k')\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('silhouette coefficient')\n",
    "plt.plot(range(2, 10), silhouette_plot)\n",
    "plt.axhline(y=np.mean(silhouette_plot), color=\"red\", linestyle=\"--\")\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## **Part 3**: **Assumptions** and **challenges** of $k$-means\n",
    "\n",
    "- Demonstrate the ideal $k$-means dataset\n",
    "- Show three scenarios where $k$-means struggles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ideal $k$-means scenario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "ideal()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Meets all assumptions:\n",
    "\n",
    "- Independent variables\n",
    "- Balanced cluster sizes\n",
    "- Clusters have similar density\n",
    "- Spherical clusters/equal variance of variables\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Problem Scenario 1 - classes not all round"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "messyOne()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Problem Scenario 2 - imbalanced class size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "messyTwo()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Problem Scenario 3 - class size and density"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "messyThree()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Solution to challenges:\n",
    "\n",
    "- Preprocessing: PCA or scaling\n",
    "- Try a different clustering methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $k$-Means Plotter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from k_means_plotter import k_means\n",
    "from sklearn.datasets import make_blobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = make_blobs(centers=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "X[:5, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X[:, 0], X[:, 1]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = k_means(X[:, 0], X[:, 1], k=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Exercise:\n",
    "### $k$-means on larger dataset - Wine subscription\n",
    "\n",
    "You want to run a wine subscription service, but you have no idea about wine tasting notes. You are a person of science.\n",
    "You have a wine dataset of scientific measurements.\n",
    "If you know a customer likes a certain wine in the dataset, can you recommend other wines to the customer in the same cluster?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Questions:\n",
    "- How many clusters are in the wine dataset?\n",
    "- What are the characteristics of each clusters?\n",
    "- What problems do you see potentially in the data?\n",
    "\n",
    "the dataset is `Wine.csv`\n",
    "\n",
    "Instructions:\n",
    "- First, remove customer_segment from the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# Work on problem here:\n",
    "wine = pd.read_csv('data/Wine.csv')\n",
    "wine.drop(columns=['Customer_Segment'], inplace=True)\n",
    "wine.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "wine.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "wine.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specifying the dataset and initializing variables\n",
    "X = wine\n",
    "distortions = []\n",
    "\n",
    "# Calculate SSE for different K\n",
    "for k in range(2, 20):\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "    kmeans.fit(X)\n",
    "    distortions.append(kmeans.inertia_)\n",
    "\n",
    "# Plot values of SSE\n",
    "plt.figure(figsize=(15, 8))\n",
    "plt.subplot(121, title='Elbow curve')\n",
    "plt.xlabel('k')\n",
    "plt.scatter(range(2, 20), distortions)\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "ss = StandardScaler().fit(wine)\n",
    "\n",
    "wine_sc = ss.transform(wine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specifying the dataset and initializing variables\n",
    "X = wine_sc\n",
    "distortions = []\n",
    "\n",
    "# Calculate SSE for different K\n",
    "for k in range(2, 10):\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "    kmeans.fit(X)\n",
    "    distortions.append(kmeans.inertia_)\n",
    "\n",
    "# Plot values of SSE\n",
    "plt.figure(figsize=(15, 8))\n",
    "plt.subplot(121, title='Elbow curve')\n",
    "plt.xlabel('k')\n",
    "plt.plot(range(2, 10), distortions)\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Review $k$-means steps\n",
    "1. Look at and clean data (if needed)\n",
    "2. Scale data\n",
    "3. Try various values of $k$\n",
    "4. Plot SSE and Silohuette coefficient to find best $k$\n",
    "5. Describe the characteristics of each cluster using their centroids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### How many clusters fit the data?\n",
    "\n",
    "What can you tell me about them?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## One last example\n",
    "\n",
    "Using online retail data data from [UCI database](https://archive.ics.uci.edu/ml/datasets/online+retail)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "You are looking for patterns so you can get people to buy more, more frequently. \n",
    "You might have to create some new variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "shopping = pd.read_excel('data/Online Retail.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "shopping.tail(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
